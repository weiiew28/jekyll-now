---
layout: post
title: A brief understanding of expressiveness of neural network
---

In Hornik[91], it is shown that if we fix an arbitrary squash function $s(x)$ that maps the real line onto the interval $(0,1)$, and use non-linear units to implement the function $s(x)$, then for every continuous mapping $f$ from $[-X,X]^{n}$ to $\mathbb{R}^n$, and arbitrary positive $e>0$, there exists a neural network that approximates $f$ with precision $e$ for all the values from $[-X,X]^n$. \\
This is the main theoretical foundation behind success of application using deep neural networks. In particular, suppose we have an oracle that gives us the query function value at arbitrary $x$, then there is an iterative learning algorithm, back-propagation that allows us to learn the weight of the neural work. For instance, $y^{l+1}=W^{l}z^{l}+b^{l}$, and $z^{l+1}=\sigma(y^{l+1})$, then $\frac{dL}{d z^{l}}=\frac{dL}{d y^{l+1}}\frac{d y^{l+1}}{d z^{l}}=(W^{l})^{T} diag(\sigma'(y^{l+1}))\frac{dL}{d z^{l+1}}$, where $\sigma(x)=\frac{1}{1+\exp(-x)}$ is the sigmoid function. Note that when $y^{l+1}$ is very positive or negative, the gradient $\sigma'(y^{l+1})$ will be very small, preventing gradient information propagating back and this is one of main difficulties of learning gradients. Later. a popular activation function called Rectified linear unit (RELU), $\max(0,x)$ is proved to be effective on many datasets. One advantage of RELU is that the gradient is constant on certain region. \\
Another interesting question is how the noise will propagate. Suppose that the original data contains some noise, then the approximate function value using the neural network is $\sigma_{k}(W_{k}...\sigma_{2}(W_{2}\sigma_{1}(W_{1}x+b_{1})+b_{2})...+b_{k})$. Assume that al the ground truth $W_{i}$s', the output values for each layer are positive, then the error at the final layer will be $W_{k}W_{k-1}\cdots W_{1}e$. If the product of the matrices has high operator norm, then it is possible that the function we will learn contains high output error. This, implies that we need to restrict the total norm of $W$'s when learning the neural network. \\
In paper Kreinovich[91], it extends the above result showing that with an arbitrary non-linear function with sufficient smoothness, the neural network model can be used to approximate arbitrary continuous function on $[-X,X]^n$. This relaxes the constraint that only those activation functions are allowed. The key proof steps are (1) using Weiestrass approximation, a polynomial approximation suffices (2) constructing the component neural network to learn product structure $xy$. (3) reduced to learn $x^2$ because $xy=\frac{1}{4}((x+y)-(x-y)^2)$. Note if we can approximate $x^2$, then we can also approximate $x^3$ using $x^3=(x^2)\cdot x$. Similarly, we can approximate arbitrary degree polynomials using a neural network. \\
I think in the proof itself, it reveals the complexity of layers and number of neurons necessary to approximate the polynomials. For $x^2$, it can be shown that 1 layer with 2 neurons is sufficient, and for $xy$, 2 layers with 4 hidden units are sufficient. Let $D(n)$ be the minimal network structure to approximate $x^n$. For a degree $n+1$ polynomial $x^{n+1}$, if it is even, then we can rewrite $x^{n+1}=(x^{\frac{n+1}{2}})^2$. Thus, an additional layer is required compared to $D(\frac{n+1}{2})$. If it is odd, we can rewrite $x^{n+1}=x^{\frac{n}{2}}\cdot x^{\frac{n}{2}+1}$, and an additional two layers are required compared to $T(\frac{n}{2})$.  In some sense, to approximate higher degree polynomials, we need larger depth. \\
But if we fix the depth,  what should we do? We first need to use lower degree polynomials to approximate the higher degrees, and the low degree polynomials only require a certain depth. One can imaging in this procedure, it is necessary that we will make our neural network wide. Let $T(l)$ be the minimal number of block units in neural network to approximate $x^l$, then we have shown above that $T(l)\approx 2T(l/2)$. This implies that we need roughly $\log(l)$ layers to approximate a degree $l$ polynomial. However, if we use only one layer, then $l$ units would be necessary. This coincides with one theorem I have seen before showing that to approximate a function using $l+1$layers would possibly require exponentially many more hidden units with only $l$ layers. \\
Deeper neural network is expressive with fewer units. However, it also makes training harder due to the vanishing or exploding gradients. 

Useful Reference:
* Hornik, Kurt. "Approximation capabilities of multilayer feedforward networks." Neural networks 4.2 (1991): 251-257.
* Kreinovich, Vladik Ya. "Arbitrary nonlinearity is sufficient to represent all functions by neural networks: a theorem." Neural networks 4.3 (1991): 381-383.
---
