--- 
layout: post 
title: A first Dive into the 10-step EM 
categories: [Machine Learning] 
tags: [Unsupervised Learning, Algorithm, Research]
---
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>A first Dive into the 10-step EM</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>The Expectation-Maximization <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization </a> Algorithm is a classical approach to solve for latent variable models. Examples include Gaussian mixtures, Hidden Markov Model and Missing data problem. In the past, our understanding of the EM algorithm is limited to its qualitative properties. For instance, it is well known that the EM algorithm is a minimization-maximization process, thus the objective value is non-decreasing. It is also well known that the EM steps converge to a local stationary point. However, our quantitative understanding is far from complete. Does the EM algorithm converge to a good local solution? How fast does it converge? The <a href="https://arxiv.org/abs/1609.00368">paper</a> “Ten steps of EM suffice for mixtures of two Gaussians” written by Daskalakis et al in 2016 provides the first global quantitative result for they dynamics for EM algorithm in the mixture of two Gaussian instance. It has inspired lots of my thoughts as well as my work to further extend their insights.</p>
 <!-- more -->
</body>
</html>
